_target_: src.models.diffusion.LatentDiffusionModule

use_ema: true
num_timesteps: 1000

optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: 1e-4
  weight_decay: 1e-5

scheduler:
  _target_: torch.optim.lr_scheduler.CosineAnnealingLR
  _partial_: true
  T_max: 100
  eta_min: 1e-7

# encoder_path: ${paths.root_dir}/src/ckpt/vq_vae64/encoder.pth # both
# decoder_path: ${paths.root_dir}/src/ckpt/vq_vae64/decoder.pth # both
# vq_layer_path: ${paths.root_dir}/src/ckpt/vq_vae64/vq_layer_512.pth # both
# model_path: ${paths.root_dir}/src/ckpt/latent_diffusion/unet_ldm64.pth

vae_net_path: None
encoder_path: ${paths.root_dir}/src/ckpt_s256/vq_vae/encoder.pth 
decoder_path: ${paths.root_dir}/src/ckpt_s256/vq_vae/decoder.pth 
vq_layer_path: ${paths.root_dir}/src/ckpt_s256/vq_vae/vq_layer_1024.pth 
model_path: ${paths.root_dir}/src/ckpt_s256/latent_diffusion/unet_ldm64_1000step.pth 

# model_path: ${paths.root_dir}/src/ckpt/latent_diffusion/unet_ldm64_v2.pth
# encoder_path: ${paths.root_dir}/src/ckpt/vq_vae/healthy/encoder.pth # healthy only
# decoder_path: ${paths.root_dir}/src/ckpt/vq_vae/healthy/decoder.pth # healthy only
# vq_layer_path: ${paths.root_dir}/src/ckpt/vq_vae/healthy/vq_layer.pth # healthy only

vae:
  _target_: src.models.vae.net.VQVAE

  encoder:
    _target_: src.models.components.up_down.Encoder

    in_channels: 4  # Adjusted to match BRATS2020 dataset
    z_channels: ${model.vae.latent_dims[0]}
    base_channels: 64
    block: Residual
    n_layer_blocks: 1
    drop_rate: 0.0
    channel_multipliers: [1, 2, 4]
    attention: Attention
    n_attention_heads: null
    n_attention_layers: null
    double_z: false

  decoder:
    _target_: src.models.components.up_down.Decoder

    out_channels: ${model.vae.encoder.in_channels}
    z_channels: ${model.vae.latent_dims[0]}
    base_channels: ${model.vae.encoder.base_channels}
    block: ${model.vae.encoder.block}
    n_layer_blocks: ${model.vae.encoder.n_layer_blocks}
    drop_rate: ${model.vae.encoder.drop_rate}
    channel_multipliers: ${model.vae.encoder.channel_multipliers}
    attention: ${model.vae.encoder.attention}
    n_attention_heads: ${model.vae.encoder.n_attention_heads}
    n_attention_layers: ${model.vae.encoder.n_attention_layers}

  latent_dims: [4, 64, 64]  # Adjusted to match the configuration

  vq_layer:
    _target_: src.models.vae.net.VectorQuantizer
    num_embeddings: 1024
    embedding_dim: ${model.vae.latent_dims[0]}
    beta: 0.25

# # VAE configuration (Beta-VAE based)
# vae:
#   _target_: src.models.vae.net.VanillaVAE

#   kld_weight: 0.0005 # al_img.shape[0]/ self.num_train_images

#   encoder:
#     _target_: src.models.components.up_down.Encoder

#     in_channels: 4  # Adjusted to match BRATS2020 dataset
#     z_channels: 4
#     base_channels: 64
#     block: Residual
#     n_layer_blocks: 1
#     drop_rate: 0.1
#     channel_multipliers: [1, 2, 4]
#     attention: Attention
#     n_attention_heads: null
#     n_attention_layers: null
#     double_z: true

#   decoder:
#     _target_: src.models.components.up_down.Decoder

#     out_channels: 4  # Changed from ${..vae.encoder.in_channels} to explicit value
#     z_channels: 4
#     base_channels: 64  # Changed from ${..vae.encoder.base_channels} to explicit value
#     block: Residual  # Changed from ${..vae.encoder.block} to explicit value
#     n_layer_blocks: 1  # Changed from ${..vae.encoder.n_layer_blocks} to explicit value
#     drop_rate: 0.1  # Changed from ${..vae.encoder.drop_rate} to explicit value
#     channel_multipliers: [1, 2, 4]  # Changed from ${..vae.encoder.channel_multipliers} to explicit value
#     attention: Attention  # Changed from ${..vae.encoder.attention} to explicit value
#     n_attention_heads: null  # Changed from ${..vae.encoder.n_attention_heads} to explicit value
#     n_attention_layers: null  # Changed from ${..vae.encoder.n_attention_layers} to explicit value

#   latent_dims: [4, 64, 64]  # Beta-VAE latent dimensions