_target_: src.models.vae.VAEModule

use_ema: true

optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: 1e-4
  weight_decay: 1e-5

scheduler:
  _target_: torch.optim.lr_scheduler.CosineAnnealingLR
  _partial_: true
  T_max: 50
  eta_min: 1e-6

encoder_path: /home/tqlong/qtung/gen-model-boilerplate/src/ckpt_lits/vq_vae/encoder.pth
decoder_path: /home/tqlong/qtung/gen-model-boilerplate/src/ckpt_lits/vq_vae/decoder.pth
vq_layer_path: /home/tqlong/qtung/gen-model-boilerplate/src/ckpt_lits/vq_vae/vq_layer_1024.pth

net:
  _target_: src.models.vae.net.VQVAE

  encoder:
    _target_: src.models.components.up_down.Encoder

    in_channels: 1
    z_channels: ${model.net.latent_dims[0]}
    base_channels: 64
    block: Residual
    n_layer_blocks: 1
    drop_rate: 0.1
    channel_multipliers: [1, 2, 4]
    attention: Attention
    n_attention_heads: null
    n_attention_layers: null
    double_z: false

  decoder:
    _target_: src.models.components.up_down.Decoder

    out_channels: ${model.net.encoder.in_channels}
    z_channels: ${model.net.latent_dims[0]}
    base_channels: 64
    block: Residual
    n_layer_blocks: 1
    drop_rate: 0.1
    channel_multipliers: [1, 2, 4]
    attention: Attention
    n_attention_heads: null
    n_attention_layers: null

  latent_dims: [1, 64, 64]  # Adjusted to match the configuration

  vq_layer:
    _target_: src.models.vae.net.VectorQuantizer
    num_embeddings: 1024
    embedding_dim: ${..latent_dims[0]} # Relative path to parent's latent_dims
    beta: 0.25

criterion: 
  _target_: torch.nn.MSELoss